{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalProject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rEAbOgvzm4XB"
      },
      "source": [
        "# SNLP 2021 Final Project\n",
        "\n",
        "Name 1:  hacane hechehouche <br/>\n",
        "Student id 1:  2571617 <br/>\n",
        "Email 1:  s8hahech@stud.uni-saarland.de <br/>\n",
        "\n",
        "\n",
        "Name 2: Tanjim Ul Haque <br/> \n",
        "Student id 2:  2577737 <br/>\n",
        "Email 2: s8tahaqi@stud.uni-saarland.de <br/> \n",
        "\n",
        "**Instructions:** Your final submission should contain _a separate Notebook_ (not including our instructions). You may also implement your code in different Python files and show the output in the Notebooks. In this case, make sure you submit your Python files as well. Do not submit the data files and any other debug output. Your submission should have a clear structure and should be easy to follow. Other instructions to follow have been given in detail with the project problem statement. Read them carefully.\n",
        "\n",
        "Upload the zipped folder in Teams. Only one member from the group should make the submission. The deadline for the project submission is August 20th, 2021 (23:59 pm). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KbPjlMwApWKb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b812fd2-e1f3-47ad-e75a-34d094e6d075"
      },
      "source": [
        "from typing import List, Dict\n",
        "import re\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fR7Wsxu2yGGq"
      },
      "source": [
        "def preprocess(file):\n",
        "  text = \"\"\n",
        "  with open(file) as f:\n",
        "    for line in f:\n",
        "        if not line.isspace():\n",
        "            text+=line.lstrip()\n",
        "\n",
        "  return text\n",
        "\n",
        "\n",
        "def train_test_split_data(text:List, test_size:float=0.2):\n",
        "\tindex=round(test_size*len(text))\n",
        "\tprint(\"size of tokens is = \"+ str(len(text)) + \" and train part is from index 0 to \" +str(index-1)  )\n",
        "\treturn text[:index],text[index:]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NjacJ8G7jct"
      },
      "source": [
        "### English Text Preprocess & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2f5vQxRhxT9z"
      },
      "source": [
        "english_doc = \"/content/drive/MyDrive/SNLP21/FinalProject/data/alice_in_wonderland.txt\"\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iSE8jn6SyV_R"
      },
      "source": [
        " englishPre= preprocess(english_doc)\n",
        " englishTrain, englishTest = train_test_split_data(englishPre)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "feNxrl-u3MaQ"
      },
      "source": [
        "  # Storing Text as a .txt file\n",
        "  \n",
        "  f = open(\"/content/drive/MyDrive/SNLP21/FinalProject/data/englishTrain.txt\", \"w\")\n",
        "  f.write(englishTrain)\n",
        "  f.close()\n",
        "\n",
        "  f = open(\"/content/drive/MyDrive/SNLP21/FinalProject/data/englishTest.txt\", \"w\")\n",
        "  f.write(englishTest)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQxQM62o7052"
      },
      "source": [
        "### Bengali Text Preprocess & Splitting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u9-glE474I2"
      },
      "source": [
        "bengali_doc = \"/content/drive/MyDrive/SNLP21/FinalProject/data/bengali_corpus.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wGzqSumZ8IRd",
        "outputId": "b1af965c-d8a9-4bc5-da67-f97029491098"
      },
      "source": [
        "banglaPre= preprocess(bengali_doc)\n",
        "banglaTrain, banglaTest = train_test_split_data(banglaPre)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "size of tokens is = 1496201 and train part is from index 0 to 299239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0X8wSZk08cxa"
      },
      "source": [
        "  # Storing Text as a .txt file\n",
        "  \n",
        "  f = open(\"/content/drive/MyDrive/SNLP21/FinalProject/data/banglaTrain.txt\", \"w\")\n",
        "  f.write(banglaTrain)\n",
        "  f.close()\n",
        "\n",
        "  f = open(\"/content/drive/MyDrive/SNLP21/FinalProject/data/banglaTest.txt\", \"w\")\n",
        "  f.write(banglaTest)\n",
        "  f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQkjo8R68mUu"
      },
      "source": [
        "### Subword segmentation for English"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aLvjcSHnryYT",
        "outputId": "ec53624d-b774-4103-ce91-4ea2735d4094"
      },
      "source": [
        "!pip install sentencepiece"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l\r\u001b[K     |▎                               | 10 kB 21.9 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 28.5 MB/s eta 0:00:01\r\u001b[K     |▉                               | 30 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 34.0 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 51 kB 20.8 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 61 kB 22.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 71 kB 14.8 MB/s eta 0:00:01\r\u001b[K     |██▏                             | 81 kB 16.1 MB/s eta 0:00:01\r\u001b[K     |██▍                             | 92 kB 14.9 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 102 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███                             | 112 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███▎                            | 122 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 133 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███▉                            | 143 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████                            | 153 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████▎                           | 163 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 174 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 184 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 194 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 204 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 215 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████                          | 225 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 235 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 245 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 256 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████                         | 266 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 276 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 286 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 296 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████                        | 307 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 317 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 327 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 337 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 348 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 358 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 368 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 378 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 389 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 399 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 409 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 419 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████▍                    | 430 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 440 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 450 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 460 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 471 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 481 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 491 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 501 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 512 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 522 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 532 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 542 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 552 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 563 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 573 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 583 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 593 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 604 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 614 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 624 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 634 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 645 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 655 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 665 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 675 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 686 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 696 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 706 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 716 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 727 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 737 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 747 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 757 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 768 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 778 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 788 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 798 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 808 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 819 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 829 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 839 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 849 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 860 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 870 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 880 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 890 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 901 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 911 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 921 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 931 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 942 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 952 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 962 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 972 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 983 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 993 kB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.0 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.0 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.0 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.0 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.0 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.1 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.2 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.2 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.2 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.2 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.2 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.2 MB 15.9 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.2 MB 15.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.96\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIjX9cYsBVCr"
      },
      "source": [
        "import sentencepiece as spm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4GecvT567TJ1"
      },
      "source": [
        "english_train = \"/content/drive/MyDrive/SNLP21/FinalProject/data/englishTrain.txt\"\n",
        "file = open(english_train, \"r\")\n",
        "texts = file.read()\n",
        "# finding the total character \n",
        "\n",
        "charlist= []\n",
        "for c in texts:\n",
        "  if c not in charlist:\n",
        "     charlist.append(c)\n",
        "\n",
        "charnum = len(charlist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8IlwJslAPPs"
      },
      "source": [
        "#train with character level vocalbulary\n",
        "spm.SentencePieceTrainer.train('--input='+str(english_train)+' --model_prefix=english_char --vocab_size='+str(charnum)+' --model_type=bpe')\n",
        "\n",
        "#train with smaller vocalbulary\n",
        "spm.SentencePieceTrainer.train('--input='+str(english_train)+' --model_prefix=english_sv --vocab_size=500 --character_coverage=1.0 --model_type=bpe')\n",
        "  \n",
        "#train with larger vocalbulary\n",
        "spm.SentencePieceTrainer.train('--input='+str(english_train)+' --model_prefix=english_lv --vocab_size=2400 --character_coverage=1.0 --model_type=bpe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CHd5AZQGysR"
      },
      "source": [
        "def Encoder(model,input, output):\n",
        "  sp = spm.SentencePieceProcessor(model_file=model)\n",
        "  out=''\n",
        "  f1 = open(output, \"w\")\n",
        "\n",
        "  #Encoding\n",
        "  with open(input) as f:\n",
        "    for sent in f:\n",
        "      out= sp.encode(sent, out_type=str)\n",
        "      f1.write(' '.join(out))\n",
        "      f1.write('\\n')\n",
        "  \n",
        "  f1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-PC0a8-Hpb7"
      },
      "source": [
        "english_train = \"/content/drive/MyDrive/SNLP21/FinalProject/data/englishTrain.txt\"\n",
        "ens1 = \"/content/drive/MyDrive/SNLP21/FinalProject/data/en_s1.txt\"\n",
        "ens2 = \"/content/drive/MyDrive/SNLP21/FinalProject/data/en_s2.txt\"\n",
        "ens3 = \"/content/drive/MyDrive/SNLP21/FinalProject/data/en_s3.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlPIXnepHfAx"
      },
      "source": [
        "Encoder('english_char.model', english_train, ens1)\n",
        "Encoder('english_sv.model', english_train, ens2)\n",
        "Encoder('english_lv.model', english_train, ens3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kq_jDmWePi5-"
      },
      "source": [
        "### Subword segmentation for Bengali\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6keniByPsfW"
      },
      "source": [
        "bangla_train = \"/content/drive/MyDrive/SNLP21/FinalProject/data/banglaTrain.txt\"\n",
        "file = open(bangla_train, \"r\")\n",
        "textB = file.read()\n",
        "# finding the total character \n",
        "\n",
        "charlistB = []\n",
        "for c in textB:\n",
        "  if c not in charlistB:\n",
        "     charlistB.append(c)\n",
        "\n",
        "charnumB = len(charlistB)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kAU06y1iP_T_"
      },
      "source": [
        "#train with character level vocalbulary\n",
        "spm.SentencePieceTrainer.train('--input='+str(bangla_train)+' --model_prefix=bangla_char --vocab_size='+str(charnumB)+' --model_type=bpe')\n",
        "\n",
        "#train with smaller vocalbulary\n",
        "spm.SentencePieceTrainer.train('--input='+str(bangla_train)+' --model_prefix=bangla_sv --vocab_size=700 --character_coverage=1.0 --model_type=bpe')\n",
        "  \n",
        "#train with larger vocalbulary\n",
        "spm.SentencePieceTrainer.train('--input='+str(bangla_train)+' --model_prefix=bangla_lv --vocab_size=1500 --character_coverage=1.0 --model_type=bpe')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KO3BKzANQmp0"
      },
      "source": [
        "bangla_train = \"/content/drive/MyDrive/SNLP21/FinalProject/data/banglaTrain.txt\"\n",
        "bns1 = \"/content/drive/MyDrive/SNLP21/FinalProject/data/bn_s1.txt\"\n",
        "bns2 = \"/content/drive/MyDrive/SNLP21/FinalProject/data/bn_s2.txt\"\n",
        "bns3 = \"/content/drive/MyDrive/SNLP21/FinalProject/data/bn_s3.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYbyORCvP_o1"
      },
      "source": [
        "Encoder('bangla_char.model', bangla_train, bns1)\n",
        "Encoder('bangla_sv.model', bangla_train, bns2)\n",
        "Encoder('bangla_lv.model', bangla_train, bns3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab5TO0ihEgZM"
      },
      "source": [
        "After checking different sizes we selected following vocab size for English and Bengali language;\n",
        "1. English Char level = input alphabet size, small vocab = 500, large vocab =2400\n",
        "\n",
        "2. Bengali Char level = input alphabet size, small vocab = 700, large vocab =1500\n",
        "\n",
        "For Bengali corpus the large vocab size seems to be confined to 1800. As putting more than that will throw an error. \n",
        "\n",
        "Next we encoded our texts using the model we trained earlier to generate our subwords. For each language we created 3 models so we got 6 subword segmentation files in the end. \n",
        "\n",
        "After comparing all the subwords we can understand that English corpus produced dataset with a assigned  subword granularity correctly. For character level each of the characters were separated including all the stop words as we passed the whole raw sentence as input we preffered to keep those words. For Bengali data it was a little different. As Bengali character not only consist of alphabet instead they have a lot of extra subcharacters that are not characters but are needed to make a word meaaningful there fore the character size were larger than the English and the output didn't follow the subword granularity in some cases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBlKxRE6sYiA"
      },
      "source": [
        "### RNNLM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94wgV2QkuGy_"
      },
      "source": [
        "Shell Script. It was executed in the linux terminal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3J4G6BbuSRQ"
      },
      "source": [
        "This generates the language model. Changing the hyper parameters and the training testing sets will provide with neccessary models.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asxSbndoscGH"
      },
      "source": [
        "! ./rnnlm -train /home/snlp-project-21/snlp-project-21/FinalProject/data/bn_s3\n",
        "  .txt -valid /home/snlp-project-21/snlp-project-21/FinalProject/data/banglaTe\n",
        "  st.txt -rnnlm modelbns3 -hidden 40  -rand-seed 1 -debug 2 -bptt 2 -class 500"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8wp48vCufrA"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxU51qRourOB"
      },
      "source": [
        "Shell Script (datagen.sh). It was executed in the linux terminal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u8pCC0amu3HA"
      },
      "source": [
        "Here we use the acquired models from the previous steps with the best hyper parameters. Here 1 example is shown where modelens1 is the model trained with character level subword data en_s1.txt. We generate from 10 to 10^7 data and each case is stored as ENS1_10.txt to ENS1_10000000.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHxE5OwAuyPR"
      },
      "source": [
        "! for i in {1..7}\n",
        "  do\n",
        "\t  ./rnnlm -rnnlm modelens1 -gen $((10**i)) -debug 0 > textgen/ENS1_$((10**$i)).txt\n",
        "  done"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_Q1kLUsvnwX"
      },
      "source": [
        "We will consider all ENS/BNS_100.txt data. As the data are in subword format we have to decode it to original form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0hguBA8ouo3C"
      },
      "source": [
        "def Decoder(model,input, output):\n",
        "  sp = spm.SentencePieceProcessor(model_file=model)\n",
        "  out=''\n",
        "  f1 = open(output, \"w\")\n",
        "\n",
        "  #Encoding\n",
        "  with open(input) as f:\n",
        "    for sent in f:\n",
        "      out= sp.decode(sent)\n",
        "      f1.write(out)\n",
        "  \n",
        "  f1.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oD5Jpl3alLrp"
      },
      "source": [
        "Comparing the 100.txt for ENglish we can see that the character level subword generates random word but those are not corelated to each water. Where for small vocab or larger vocab size the words generated are more sensible and it complements previous words. For Bengali corpus the understanding is not as clear as English for the structure of the language and characters. Still it some what follows the same principle that can be seen for English Corpus. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TIc6QyD620ys"
      },
      "source": [
        "### English decoded texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vk9b4tow973"
      },
      "source": [
        "originaltextE1 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/originaltext/ogEs1.txt\"\n",
        "originaltextE2 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/originaltext/ogEs2.txt\"\n",
        "originaltextE3 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/originaltext/ogEs3.txt\"\n",
        "ens1 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/ENS1/ENS1_100.txt\"\n",
        "ens2 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/ENS2/ENS2_100.txt\"\n",
        "ens3 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/ENS3/ENS3_100.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "linTFR8pwqI2"
      },
      "source": [
        "Decoder('english_char.model',ens1,originaltextE1)\n",
        "Decoder('english_sv.model',ens2,originaltextE2)\n",
        "Decoder('english_lv.model',ens3,originaltextE3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lofxO0bw24gu"
      },
      "source": [
        "### Bengali decoded texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "14wh8SM626xE"
      },
      "source": [
        "originaltextB1 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/originaltext/ogBs1.txt\"\n",
        "originaltextB2 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/originaltext/ogBs2.txt\"\n",
        "originaltextB3 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/originaltext/ogBs3.txt\"\n",
        "bns1 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/BNS1/BNS1_100.txt\"\n",
        "bns2 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/BNS2/BNS2_100.txt\"\n",
        "bns3 = \"/content/drive/MyDrive/SNLP21/FinalProject/textgen/BNS3/BNS3_100.txt\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDLigTkH3NGZ"
      },
      "source": [
        "Decoder('english_char.model',bns1,originaltextB1)\n",
        "Decoder('english_sv.model',bns2,originaltextB2)\n",
        "Decoder('english_lv.model',bns3,originaltextB3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3w7AAvOUl24K"
      },
      "source": [
        "## Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pL9jftXB4rbT"
      },
      "source": [
        "### Introduction\n",
        "\n",
        "Out of vocalbulary words are a serious issues in the language models. Out of vocalbulary words are terms that are not a part of the normal lexicons in a NLP environment. When a word is not available in the training data , but appears in the testing set. The main issue with it is the models assigns a prbaibility of zero leadins to a zero likelihood as well as lowering the model performance. Spme attempts have been made in the past to estimate representations of OOV words. One used external auxilary information where the disadvantage was that you will need an external information. Another way was to embed words that takes OOV words and maps them into a vectore representation. In this project we are analyzing the oov word embedding based on subwords on 2 language data. One is Englishe text another is Bengali language text.\n",
        "\n",
        "### Required Steps\n",
        "\n",
        "For this project work we will be requiring following steps:\n",
        "\n",
        "1.   Selecting appropiate subwords from the given corpus\n",
        "2.   Building a Language Model by training RNN over the subwords.\n",
        "\n",
        "3. Generate larger corpus using the language models\n",
        "\n",
        "4. Compareing OOV rates and performing hyperparameter tuning\n",
        "\n",
        "### Data Preprocessing\n",
        "\n",
        "As we are using a tool named sentencePiece to select subwords from a corpus we need to pass raw sentence as an input. Therefore we need to process our dataset. For both language data we select each sentence and store it in a file separating with a signle line. After preprocessing we are splitting the total data in to 80-20 ratio of training and testing data.  We store it as train.txt and test.txt for each language.\n",
        "\n",
        "### Subword Segmentation\n",
        "\n",
        "We are using the SentencePiece implementation of BPE. At first we are training the sentencePiece model with a specified vocabulary size. For our task we consider 3 types of subword granularity, character level, small vocab, large vocab. After checking different sizes we selected following vocab size for English and Bengali language;\n",
        "1. English Char level = input alphabet size, small vocab = 500, large vocab =2400\n",
        "\n",
        "2. Bengali Char level = input alphabet size, small vocab = 700, large vocab =1500\n",
        "\n",
        "For Bengali corpus the large vocab size seems to be confined to 1800. As putting more than that will throw an error. \n",
        "\n",
        "Next we encoded our texts using the model we trained earlier to generate our subwords. For each language we created 3 models so we got 6 subword segmentation files in the end. \n",
        "\n",
        "After comparing all the subwords we can understand that English corpus produced dataset with a assigned  subword granularity correctly. For character level each of the characters were separated including all the stop words as we passed the whole raw sentence as input we preffered to keep those words. For Bengali data it was a little different. As Bengali character not only consist of alphabet instead they have a lot of extra subcharacters that are not characters but are needed to make a word meaaningful there fore the character size were larger than the English and the output didn't follow the subword granularity in some cases. \n",
        "\n",
        "### Language Model using RNN\n",
        "\n",
        "In this step we are training 3 language models for each language from the earlier corpora. We are using RNNLM toolkit. The perforemance depends on number of hidden latets, back propagation parameters. For implementing class based language model we use class size. After training the RNNNLM outputs perplexity of the training model. We tune the parameters to achieve perplexitiy below than baseline. We can see the perplexity for each language below,\n",
        "\n",
        "#### English Corpora\n",
        "\n",
        "  * baseline s1 Train Entropy 3.2338 Valid Entropy 5.408 alpha =.025\n",
        "  * baselone s2 train Entropy 7.6375 Valid Entropy 8.1982 alpha =.05\n",
        "  * baseline s3 train Entropy 7.6280 valid Entropy 9.3615 alpha =.05\n",
        "  \n",
        "\n",
        "Our results ( increased hidden layers to 128, selecting classsize as 200,500,900)\n",
        "\n",
        "* tuned s1 Train Entropy 3.2128 Valid Entropy 5.3745 alpha =.05\n",
        "* tuned s2 train Entropy 7.6315 Valid Entropy 7.9883 alpha =.025\n",
        "* tuned s3 train Entropy 7.5352 valid Entropy 9.2545 alpha =.025\n",
        "\n",
        "#### Bengali Corpora\n",
        "\n",
        "  * baseline s1 Train Entropy 4.3710 Valid Entropy 8.0988 alpha =.025\n",
        "  * baselone s2 train Entropy 5.7609 Valid Entropy 9.8194 alpha =.05\n",
        "  * baseline s3 train Entropy 7.6280 valid Entropy 9.3615 al[ha = 0.003\n",
        "\n",
        "\n",
        "Our results ( increased hidden layers to 128, selecting classsize as 200,500,900)\n",
        "\n",
        "* tuned s1 Train Entropy 3.9922 Valid Entropy 8.0063  alpha =.00625\n",
        "* tuned s2 train Entropy 5.7361 Valid Entropy 9.6414  alpha =.025\n",
        "* tuned s3 train Entropy 6.5691 valid Entropy 10.6734 alpha =.025\n",
        "\n",
        "\n",
        "### Text Generation\n",
        "\n",
        "\n",
        "For text generation we are using rnnlm toolkit. They include generating function where we are inputting the LM models and the size and it will generate accordingly. After generation it still stays as a subword so we are decoding it to get the original text. Comparing the 100.txt for ENglish we can see that the character level subword generates random word but those are not corelated to each water. Where for small vocab or larger vocab size the words generated are more sensible and it complements previous words. \n",
        "\n",
        "### OOV Comparison\n",
        "\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "So what we have understand from doing the project is how we can use subword level embeddings for representing OOV words. We have analyzed 2 different language corpus and applied 3 level of subword granularity to each of them. After getting the subword corpora we trained a language model over it and generated some rnadom texts with the model. After comparing the generated words we understood that the vocabulary level subword contains more meaningful relations than character level subwords. We than consider original data as well as the genrated data to compare the OOV rates. \n",
        "\n",
        "\n"
      ]
    }
  ]
}